{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FK8S4C6kG3d"
   },
   "source": [
    "#User-Movie Mapping Sparse Matrix Dataset\n",
    "**No need to execute if dataset_matrix.npz already present**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwqFqla9i55Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "def save_sparse_matrix(filename, x):\n",
    "    x_coo = x.tocoo()\n",
    "    row = x_coo.row\n",
    "    col = x_coo.col\n",
    "    data = x_coo.data\n",
    "    shape = x_coo.shape\n",
    "    np.savez(filename, row=row, col=col, data=data, shape=shape)\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    y = np.load(filename)\n",
    "    z = sparse.coo_matrix((y['data'], (y['row'], y['col'])), shape=y['shape'])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92yKigXMi9IR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ratings_df = pd.read_csv('clean/rating_updated_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-3GPQuqi_kT"
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('clean/movies_clean.csv')\n",
    "movies_df.iloc[0].movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhennMWmi2H_"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from tqdm import tqdm\n",
    "user_movie = lil_matrix((ratings_df['userId'].unique().shape[0]+1, movies_df.shape[0]))\n",
    "for i in tqdm(range(0,movies_df.shape[0])):\n",
    "  movies = ratings_df[ratings_df.movieId == movies_df.iloc[i]['movieId']]\n",
    "  userIdList = movies['userId'].values\n",
    "  for j in userIdList:\n",
    "        user_movie[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlM2_At_kt2p"
   },
   "outputs": [],
   "source": [
    "save_sparse_matrix('tmp/dataset_matrix',user_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bo0klhIlhz2"
   },
   "outputs": [],
   "source": [
    "z = load_sparse_matrix('tmp/dataset_matrix.npz').tolil()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etFBgsydX17d"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#<h2>HERE LIES THE HVAE CODE THE ONE AND THE ONLY CODE</h2>\n",
    "\n",
    "---\n",
    "Download Required:\n",
    "\n",
    "[PreTrained Weights](https://drive.google.com/file/d/1ikZOBA46TEZJLm88lsuXPG8bqtC7eJU4/view)\n",
    "\n",
    "[User-Movie Sparse Matrix dataset.npz](https://drive.google.com/file/d/1onaqEkTF-Fo7iHTztcJUrep1l5Ht5rz6/view)\n",
    "\n",
    "[Embed Matrix](https://drive.google.com/file/d/1YF4BGBIklBRso-7rAmYkccT9REVTNXbK/view)\n",
    "\n",
    "[Movies Dataset for movie title and index](https://drive.google.com/file/d/1-BvShIGsXyWzvQ_ssXqp9E5wnbnxSXA7/view)\n",
    "\n",
    "Instruction to Execute:\n",
    "1. Download necessary dataset and matrices. Update below cell and fix path according to downloaded data path. \n",
    "2. Execute First Cell to load required datasest, matrices and define VAE model architecture and loss function\n",
    "3. Load PreSaved Weights from checkpoint\n",
    "4. Generate Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7CNKhw2TjSG"
   },
   "source": [
    "### <h1>Complete HVAE CODE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EJCGrJepQakr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "  Load Movies Dataset (62000k)\n",
    "  Load [User -> Movie Map] : [162k,62K] Sparse Matrix \n",
    "  Load [Embed Movie Feature Vector] : [62k, 3] Embedding generated from MVAE\n",
    "'''\n",
    "movies_df = pd.read_csv('clean/movies_clean.csv')\n",
    "\n",
    "embed_movie_feature = np.load('tmp/embed_movie.npy')\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "def save_sparse_matrix(filename, x):\n",
    "    x_coo = x.tocoo()\n",
    "    row = x_coo.row\n",
    "    col = x_coo.col\n",
    "    data = x_coo.data\n",
    "    shape = x_coo.shape\n",
    "    np.savez(filename, row=row, col=col, data=data, shape=shape)\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    y = np.load(filename)\n",
    "    z = sparse.coo_matrix((y['data'], (y['row'], y['col'])), shape=y['shape'])\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "x_train = load_sparse_matrix('tmp/dataset_matrix.npz').tolil()\n",
    "\n",
    "# Convert to CSR format from stored COO format remove initial empty \n",
    "x_train = x_train.tocsr()\n",
    "x_train = x_train[1:]\n",
    "\n",
    "\n",
    "'''\n",
    "  VAE Architecture Design\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding movie.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    \"\"\"Maps movie vector to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, vocab_size=1000, embed_dim=3, seq_length=1000, weights=[],name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"tanh\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.embedding_layer = layers.Embedding(vocab_size,embed_dim ,weights=weights, input_length=seq_length, trainable=True)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed = self.embedding_layer(inputs)\n",
    "        flat_embed = self.flatten_layer(embed)\n",
    "        x = self.dense_proj(flat_embed)\n",
    "        # x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    \"\"\"Converts z, the encoded movie vector, back into a movie feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"tanh\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        vocab_size=1000,\n",
    "        embed_dim=3,\n",
    "        weights=[],\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim, vocab_size=vocab_size, embed_dim=embed_dim, seq_length=vocab_size, weights=weights)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        ,axis=1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "\n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQrY1WIfVxWr"
   },
   "source": [
    "# <h2>Training is Expensive Especially for Currennt X_train dataset containing [162k,62k] matrix. Load Model With  PreSaved Weights for evaluation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZqSIe65Qgpc"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "'''\n",
    "  Multiprocessing for managing GPU RAM in Google Colab for deallocation of GPU Usage\n",
    "  fit_generator(deprecated) generator useful when training large dataset when entire dataset cannot fit in RAM\n",
    "  fit used for training small dataset that can fit in RAM\n",
    "  IN TF2 fit function can take generator. fit_generator hence deprecated.\n",
    "  Train Using model.fit(generator(x,y,batch_size), ... )\n",
    "'''\n",
    "def create_model_and_train():\n",
    "\n",
    "    SAMPLES_PER_EPOCH = x_train.shape[0]\n",
    "    INPUT_DIM = x_train.shape[1] \n",
    "    INTERMEDIATE_DIM = 600\n",
    "    LATENT_DIM = 200\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-3\n",
    "    VOCAB_SIZE = x_train.shape[1]\n",
    "    EMBED_DIM = 3\n",
    "    STEPS_PER_EPOCH = np.math.ceil(SAMPLES_PER_EPOCH/BATCH_SIZE)\n",
    "\n",
    "    # checkpoint_path = '/content/tmp/training/cp-{epoch:04d}.ckpt'\n",
    "    checkpoint_path = '/content/drive/My Drive/Colab Notebooks/Data/tmp/training/cp-{epoch:04d}.h5'\n",
    "\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    SAVE_PERIOD = 5\n",
    "\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, save_freq='epoch',period=SAVE_PERIOD,\n",
    "        monitor='loss',\n",
    "        mode='auto',\n",
    "         save_best_only=True)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    vae = VariationalAutoEncoder( original_dim=INPUT_DIM,\n",
    "            intermediate_dim=INTERMEDIATE_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            weights=[embed_movie_feature])\n",
    "    # train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))\n",
    "    vae.compile(optimizer, loss=custom_sigmoid_cross_entropy_loss_with_logits)\n",
    "\n",
    "\n",
    "\n",
    "    vae.fit(nn_batch_generator(x_train, x_train, BATCH_SIZE, SAMPLES_PER_EPOCH) , steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "p = multiprocessing.Process(target=create_model_and_train())\n",
    "p.start()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbJ_PwzWXOSR"
   },
   "outputs": [],
   "source": [
    "# Save in H5 format for compact file\n",
    "new_vae.save_weights('/content/drive/My Drive/Colab Notebooks/Data/tmp/model/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmY67O2fRxPs"
   },
   "source": [
    "# <h2>Load PreSaved Weights from checkpoint</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7iE9zxjoQgqx"
   },
   "outputs": [],
   "source": [
    "\n",
    "SAMPLES_PER_EPOCH = x_train.shape[0]\n",
    "INPUT_DIM = x_train.shape[1] \n",
    "INTERMEDIATE_DIM = 600\n",
    "LATENT_DIM = 200\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "VOCAB_SIZE = x_train.shape[1]\n",
    "EMBED_DIM = 3\n",
    "STEPS_PER_EPOCH = np.math.ceil(SAMPLES_PER_EPOCH/BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "new_vae = VariationalAutoEncoder( original_dim=INPUT_DIM,\n",
    "            intermediate_dim=INTERMEDIATE_DIM,\n",
    "            latent_dim=LATENT_DIM,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            weights=[embed_movie_feature])\n",
    "    # train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))\n",
    "new_vae.compile(optimizer, loss=custom_sigmoid_cross_entropy_loss_with_logits)\n",
    "\n",
    "''''\n",
    "  Above model creation same for both training and loading\n",
    "  Below  load model weights from saved checkpoint\n",
    "'''\n",
    "\n",
    "# pass input shape for initial build(necessary if no input shape defined i.e. Input Layer not defined  in mode)\n",
    "new_vae.predict(np.array(x_train[0].todense()).reshape(1,62000))\n",
    "\n",
    "# checkpoint = new_vae.load_weights(\"/content/drive/My Drive/Colab Notebooks/Data/tmp/training/cp-0005.ckpt\")\n",
    "\n",
    "checkpoint = new_vae.load_weights(\"saved_model/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muHtk2kCRfFV"
   },
   "source": [
    "# <h2>Generate Predictions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDpESpBVRfGM",
    "outputId": "af05c16e-4169-4dea-ef06-bfd85cebfd6c"
   },
   "outputs": [],
   "source": [
    "k=20\n",
    "\n",
    "user_ID_rand = int(np.random.randint(610, size=1))\n",
    "\n",
    "test = np.array(x_train[user_ID_rand].todense()).reshape((1,INPUT_DIM))\n",
    "_,_,z=new_vae.encoder(test)\n",
    "test_reconstructed = new_vae.decoder(z)\n",
    "test_reconstructed = test_reconstructed.numpy()\n",
    "\n",
    "top_rated_movies_idx = [i for i, x in enumerate(test[0].tolist()) if x == 1.0]\n",
    "print(f'User liked {len(top_rated_movies_idx)} movies')\n",
    "\n",
    "if len(top_rated_movies_idx) == 0:\n",
    "  print('Emptylist')\n",
    "else:\n",
    "  # print(top_rated_movies_idx)\n",
    "  sorted_ratings = test_reconstructed[0].tolist()\n",
    "\n",
    "  top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "  # print(top_predicted_movies_idx)\n",
    "\n",
    "print('\\nLiked movies indices')\n",
    "count=0\n",
    "for i in top_rated_movies_idx:\n",
    "  print(movies_df.iloc[i]['movieId'], end= ' ')\n",
    "  count += 1\n",
    "  if count >10:\n",
    "    break\n",
    "print() \n",
    "print('\\nPredicted Movies indices')\n",
    "for i in top_predicted_movies_idx:\n",
    "  print(movies_df.iloc[i]['movieId'], end= ' ')\n",
    "print() \n",
    "\n",
    "print('\\nLiked Movies\\n')\n",
    "count=0\n",
    "for i in top_rated_movies_idx:\n",
    "\n",
    "  print(movies_df[movies_df.movieId == movies_df.iloc[i]['movieId']]['title'].values, end= '->')\n",
    "  print(movies_df[movies_df.movieId ==  movies_df.iloc[i]['movieId']]['genres'].values)\n",
    "  count += 1\n",
    "  if count >10:\n",
    "    break\n",
    "  # print(movies_df[movies_df.movieId ==  movie_dataset_df.columns[i]].head())\n",
    "  # print(i)\n",
    "  # print(movie_dataset_df.columns[i])\n",
    "print('*'*100)\n",
    "\n",
    "print('\\nPredicted Movies\\n')\n",
    "for i in top_predicted_movies_idx:\n",
    "  print(movies_df[movies_df.movieId == movies_df.iloc[i]['movieId']]['title'].values, end= '->')\n",
    "  print(movies_df[movies_df.movieId ==  movies_df.iloc[i]['movieId']]['genres'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-FK8S4C6kG3d"
   ],
   "name": "HVAE(Movie Recommendation).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
