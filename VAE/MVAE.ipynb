{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('clean/MovieFeatureVector.csv')\n",
    "feature_df.pop('tmdbId')\n",
    "feature_df = feature_df.set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Index in  movie feature vector</h2>\n",
    "<ul>\n",
    "<li>0:106 Languages</li>\n",
    "<li>107 adult</li>\n",
    "<li>108 voteaverage</li>\n",
    "<li>109:111 VAD</li>\n",
    "<li>112:411 Word2Vec</li>\n",
    "</ul>\n",
    "\n",
    "<p>Total dimension of movie feature vector 412</p>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(feature_df.iloc[:,0:106])\n",
    "# print(feature_df.iloc[:,106:107])\n",
    "# print(feature_df.iloc[:,107:108])\n",
    "# print(feature_df.iloc[:,108:111])\n",
    "feature_df.iloc[:,0:411]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MOVIE EMBEDDINGS</h1>\n",
    "<h2>MVAE</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_feature = feature_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_feature.shape\n",
    "INPUT_SIZE = movie_feature.shape[1]\n",
    "INTERMEDIATE_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "# tfd = tfp.distributions\n",
    "# tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(tfk.Model):\n",
    "    \n",
    "    def __init__(self, dim_z, kl_weight=1, name=\"autoencoder\", **kwargs):\n",
    "        super(MVAE, self).__init__(name=name, **kwargs)\n",
    "        self.dim_x = INPUT_SIZE\n",
    "        self.dim_z = dim_z\n",
    "        self.kl_weight = kl_weight\n",
    "        self.initializer = tfk.initializers.HeNormal()\n",
    "        self.encoder = self.encoder_z()\n",
    "        self.decoder = self.decoder_x()\n",
    "    # Sequential API encoder\n",
    "    def encoder_z(self):\n",
    "        layers = [tfkl.InputLayer(input_shape=self.dim_x)]\n",
    "        layers.append(tfkl.Dense(INTERMEDIATE_DIM,kernel_initializer=self.initializer, activation='relu'))\n",
    "        # *2 because number of parameters for both mean and (raw) standard deviation\n",
    "        layers.append(tfkl.Dense(self.dim_z*2,kernel_initializer=self.initializer,  activation=None))\n",
    "        return tfk.Sequential(layers)\n",
    "    \n",
    "    def encode(self, x_input):\n",
    "        mu, rho = tf.split(self.encoder(x_input), num_or_size_splits=2, axis=1)\n",
    "        sd = tf.math.log(1+tf.math.exp(rho))\n",
    "        z_sample = mu + sd * tf.random.normal(shape=(self.dim_z,))\n",
    "        return z_sample, mu, sd\n",
    "    \n",
    "    # Sequential API decoder\n",
    "    def decoder_x(self):\n",
    "        layers = [tfkl.InputLayer(input_shape=self.dim_z)]\n",
    "        layers.append(tfkl.Dense(INTERMEDIATE_DIM,kernel_initializer=self.initializer,  activation='relu'))\n",
    "        layers.append(tfkl.Dense(self.dim_x,kernel_initializer=self.initializer, activation='sigmoid'))\n",
    "  \n",
    "        return tfk.Sequential(layers, name='decoder')\n",
    "    \n",
    "    def call(self, x_input):\n",
    "        z_sample, mu, sd = self.encode(x_input)\n",
    "        kl_divergence = tf.math.reduce_mean(- 0.5 * \n",
    "                tf.math.reduce_sum(1+tf.math.log(\n",
    "                tf.math.square(sd))-tf.math.square(mu)-tf.math.square(sd), axis=1))\n",
    "        x_logits = self.decoder(z_sample)\n",
    "        # VAE_MNIST is inherited from tfk.Model, thus have class method add_loss()\n",
    "        self.add_loss(self.kl_weight * kl_divergence)\n",
    "        return x_logits\n",
    "    \n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################   The following code shows how to train the model   ####################\n",
    "# set hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "latent_dim=3\n",
    "kl_w=3\n",
    "vae = VAE_MNIST(dim_z=latent_dim, kl_weight=kl_w)\n",
    "vae.encoder.summary()\n",
    "vae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "\n",
    "####################   The following code shows how to train the model   ####################\n",
    "# set hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "latent_dim=3\n",
    "kl_w=3\n",
    "# compile and train tfk.Model\n",
    "vae = MVAE(dim_z=latent_dim, kl_weight=kl_w)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "            loss=custom_sigmoid_cross_entropy_loss_with_logits, metrics=[tfk.metrics.Recall(top_k=50)])\n",
    "train_history = vae.fit(x=movie_feature, y=movie_feature, batch_size=batch_size, epochs=epochs, \n",
    "                        verbose=1,  shuffle=True, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "vae.save_weights('saved_model/my_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vae = MVAE(dim_z=latent_dim, kl_weight=kl_w)\n",
    "new_vae.built=True\n",
    "new_var = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "new_vae.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Keras Docs MOd</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies\n",
    "# ORIGINAL DIM/ INPUT SIZE\n",
    "INPUT_SIZE = movie_feature.shape[1]\n",
    "# print(INPUT_SIZE)\n",
    "INTERMEDIATE_DIM = 50\n",
    "LATENT_DIM = 3\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(INPUT_SIZE))\n",
    "x = layers.Dense(INTERMEDIATE_DIM, activation=\"relu\")(encoder_inputs)\n",
    "z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(LATENT_DIM,))\n",
    "x = layers.Dense(INTERMEDIATE_DIM,activation='relu')(latent_inputs)\n",
    "decoder_outputs = layers.Dense(INPUT_SIZE, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "# decoder.summary()\n",
    "\n",
    "\n",
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "  \n",
    "    def normal_log_pdf(self, sample, mean, var, raxis=1):\n",
    "        logvar = tf.math.log(var)\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(-.5 * ((sample-mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # total_loss = self.vae_cost(data)\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *=INPUT_SIZE\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'training_2/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "STEPS_PER_EPOCH = movie_feature.shape[0] \n",
    "SAVE_PERIOD = 5\n",
    "\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, save_freq=int(SAVE_PERIOD * STEPS_PER_EPOCH))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adagrad(1e-6))\n",
    "vae.encoder.summary()\n",
    "vae.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "history = vae.fit(movie_feature, epochs=EPOCHS, batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_train = history.history['loss']\n",
    "# loss_kl = history.history['kl_loss']\n",
    "epochs = range(EPOCHS)\n",
    "plt.plot(epochs, loss_train, 'g', label='Reconstruction loss')\n",
    "# plt.plot(epochs, loss_kl, 'b', label='KL Loss' )\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "train_images = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "test_images = np.expand_dims(x_test, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_MNIST(tfk.Model):\n",
    "    \n",
    "    def __init__(self, dim_z, kl_weight=1, name=\"autoencoder\", **kwargs):\n",
    "        super(VAE_MNIST, self).__init__(name=name, **kwargs)\n",
    "        self.dim_x = (28, 28, 1)\n",
    "        self.dim_z = dim_z\n",
    "        self.encoder = self.encoder_z()\n",
    "        self.decoder = self.decoder_x()\n",
    "        self.kl_weight = kl_weight\n",
    "        \n",
    "    # Sequential API encoder\n",
    "    def encoder_z(self):\n",
    "        layers = [tfkl.InputLayer(input_shape=self.dim_x)]\n",
    "        layers.append(tfkl.Conv2D(filters=32, kernel_size=3, strides=(2,2), \n",
    "                                  padding='valid', activation='relu'))\n",
    "        layers.append(tfkl.Conv2D(filters=64, kernel_size=3, strides=(2,2), \n",
    "                                  padding='valid', activation='relu'))\n",
    "        layers.append(tfkl.Flatten())\n",
    "        # *2 because number of parameters for both mean and (raw) standard deviation\n",
    "        layers.append(tfkl.Dense(self.dim_z*2, activation=None))\n",
    "        return tfk.Sequential(layers)\n",
    "    \n",
    "    def encode(self, x_input):\n",
    "        mu, rho = tf.split(self.encoder(x_input), num_or_size_splits=2, axis=1)\n",
    "        sd = tf.math.log(1+tf.math.exp(rho))\n",
    "        z_sample = mu + sd * tf.random.normal(shape=(self.dim_z,))\n",
    "        return z_sample, mu, sd\n",
    "    \n",
    "    # Sequential API decoder\n",
    "    def decoder_x(self):\n",
    "        layers = [tfkl.InputLayer(input_shape=self.dim_z)]\n",
    "        layers.append(tfkl.Dense(7*7*32, activation=None))\n",
    "        layers.append(tfkl.Reshape((7,7,32)))\n",
    "        layers.append(tfkl.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \n",
    "                                           padding='same', activation='relu'))\n",
    "        layers.append(tfkl.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \n",
    "                                           padding='same', activation='relu'))\n",
    "        layers.append(tfkl.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
    "                                           padding='same'))\n",
    "        return tfk.Sequential(layers, name='decoder')\n",
    "    \n",
    "    def call(self, x_input):\n",
    "        z_sample, mu, sd = self.encode(x_input)\n",
    "        kl_divergence = tf.math.reduce_mean(- 0.5 * \n",
    "                tf.math.reduce_sum(1+tf.math.log(\n",
    "                tf.math.square(sd))-tf.math.square(mu)-tf.math.square(sd), axis=1))\n",
    "        x_logits = self.decoder(z_sample)\n",
    "        # VAE_MNIST is inherited from tfk.Model, thus have class method add_loss()\n",
    "        self.add_loss(self.kl_weight * kl_divergence)\n",
    "        return x_logits\n",
    "    \n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n",
    "\n",
    "  \n",
    "####################   The following code shows how to train the model   ####################\n",
    "# set hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "latent_dim=16\n",
    "kl_w=3\n",
    "# compile and train tfk.Model\n",
    "vae = VAE_MNIST(dim_z=latent_dim, kl_weight=kl_w)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "            loss=custom_sigmoid_cross_entropy_loss_with_logits)\n",
    "train_history = vae.fit(x=train_images, y=train_images, batch_size=batch_size, epochs=epochs, \n",
    "                        verbose=1, validation_data=(test_images, test_images), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_train = train_history.history['loss']\n",
    "# loss_kl = history.history['kl_loss']\n",
    "epochs = range(10)\n",
    "plt.plot(epochs, loss_train, 'g', label='Reconstruction loss')\n",
    "# plt.plot(epochs, loss_kl, 'b', label='KL Loss' )\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Better I guess</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding movie.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps movie vector to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded movie vector, back into a movie feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        ,axis=1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "\n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=60000\n",
    "validation_size=10000\n",
    "np.random.shuffle(movie_feature)\n",
    "train_dataset= movie_feature[:train_size]\n",
    "validation_dataset =  movie_feature[train_size:train_size+validation_size]\n",
    "print(train_dataset.shape)\n",
    "validation_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_DIM = 50\n",
    "LATENT_DIM = 3\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "vae = VariationalAutoEncoder(INPUT_SIZE, INTERMEDIATE_DIM, LATENT_DIM, name='MVAE')\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "vae.compile(optimizer, loss=custom_sigmoid_cross_entropy_loss_with_logits)\n",
    "train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss_train = train_history.history['loss']\n",
    "loss_val = train_history.history['val_loss']\n",
    "epochs = range(EPOCHS)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataset:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,predicted= vae.encoder(movie_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_np = predicted.numpy()\n",
    "np.save('tmp/embed_movie',predicted_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_np_load = np.load('tmp/embed_movie.npy')\n",
    "predicted_np_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,p = vae.encoder(movie_feature[2].reshape(1,411))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HVAE Hybrid Variational Autoencoder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "def save_sparse_matrix(filename, x):\n",
    "    x_coo = x.tocoo()\n",
    "    row = x_coo.row\n",
    "    col = x_coo.col\n",
    "    data = x_coo.data\n",
    "    shape = x_coo.shape\n",
    "    np.savez(filename, row=row, col=col, data=data, shape=shape)\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    y = np.load(filename)\n",
    "    z = sparse.coo_matrix((y['data'], (y['row'], y['col'])), shape=y['shape'])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ratings_df = pd.read_csv('clean/rating_updated_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('clean/movies_clean.csv')\n",
    "movies_df.iloc[0].movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from tqdm import tqdm\n",
    "user_movie = lil_matrix((ratings_df['userId'].unique().shape[0]+1, movies_df.shape[0]))\n",
    "for i in tqdm(range(0,movies_df.shape[0])):\n",
    "  movies = ratings_df[ratings_df.movieId == movies_df.iloc[i]['movieId']]\n",
    "  userIdList = movies['userId'].values\n",
    "  for j in userIdList:\n",
    "        user_movie[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_matrix('tmp/dataset_matrix',user_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = load_sparse_matrix('tmp/dataset_matrix.npz').tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_sparse_matrix('tmp/dataset_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CSR format from stored COO format remove initial empty \n",
    "x_train = x_train.tocsr()\n",
    "x_train = x_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_movie_feature = np.load('tmp/embed_movie.npy')\n",
    "embed_movie_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sparse_matrix(sparse_matrix):\n",
    "    index = np.arange(np.shape(sparse_matrix)[0])\n",
    "    np.random.shuffle(index)\n",
    "    return sparse_matrix[index, :]\n",
    "\n",
    "def batch_sparse_matrix(sparse_matrix, batch_size=64, shuffle=True):\n",
    "    sparse_matrix = shuffle_sparse_matrix(sparse_matrix) if shuffle else sparse_matrix\n",
    "    # Check if shuflle works\n",
    "    # print(sparse_matrix[1].sum())\n",
    "    index = np.arange(np.shape(sparse_matrix)[0])\n",
    "    steps_per_epoch = int(index.shape[0]/batch_size)\n",
    "    \n",
    "    \n",
    "    for i in range(0,steps_per_epoch):\n",
    "        yield i+1, sparse_matrix[batch_size*i: batch_size*(i+1)]\n",
    "    \n",
    "    yield i+2,sparse_matrix[batch_size*(i+1): ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding movie.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    \"\"\"Maps movie vector to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, vocab_size=1000, embed_dim=3, seq_length=1000, weights=[],name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"tanh\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.embedding_layer = layers.Embedding(vocab_size,embed_dim ,weights=weights, input_length=seq_length, trainable=True)\n",
    "        self.flatten_layer = layers.Flatten()\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed = self.embedding_layer(inputs)\n",
    "        flat_embed = self.flatten_layer(embed)\n",
    "        x = self.dense_proj(flat_embed)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    \"\"\"Converts z, the encoded movie vector, back into a movie feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"tanh\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        vocab_size=1000,\n",
    "        embed_dim=3,\n",
    "        weights=[],\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim, vocab_size=vocab_size, embed_dim=embed_dim, seq_length=vocab_size, weights=weights)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        ,axis=1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "\n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = x_train.shape[1] \n",
    "INTERMEDIATE_DIM = 600\n",
    "LATENT_DIM = 200\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "VOCAB_SIZE = x_train.shape[1]\n",
    "EMBED_DIM = 3\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, x_batch_train = next(train_dataset)\n",
    "np.array(x_batch_train.todense()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = batch_sparse_matrix(x_train, 128, False)\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "vae = VariationalAutoEncoder( original_dim=INPUT_DIM,\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=EMBEDINPUT_DIM = x_train.shape[1] \n",
    "INTERMEDIATE_DIM = 600\n",
    "LATENT_DIM = 200\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "VOCAB_SIZE = x_train.shape[1]\n",
    "EMBED_DIM = 3\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))\n",
    "_DIM,\n",
    "        weights=[embed_movie_feature])\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, dataset,optimizer, epochs):\n",
    "    # Iterate over epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Start of epoch %d\" % (epoch,))\n",
    "      \n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, x_batch_train in train_dataset:\n",
    "            x_batch_train = np.array(x_batch_train.todense())\n",
    "      \n",
    "            with tf.GradientTape() as tape:\n",
    "                reconstructed = model(x_batch_train)\n",
    "                # Compute reconstruction loss\n",
    "                loss = custom_sigmoid_cross_entropy_loss_with_logits(x_batch_train, reconstructed)\n",
    "                loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "            grads = tape.gradient(loss, vae.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "\n",
    "            if step % 1 == 0:\n",
    "                print(\"step %d: mean loss = %.4f\" % (step, loss.eval()))\n",
    "                \n",
    "train_step(vae, train_dataset, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# def func(row):\n",
    "#     return row\n",
    "\n",
    "# new_df = ratings_df.iloc[0:1000000].progress_apply(func,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdList = ratings_df['userId'].unique()\n",
    "userIdList.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie = np.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_df.to_csv('clean/rating_updated_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movies_dict = {}\n",
    "\n",
    "for i in tqdm(userIdList[0:10]):\n",
    "    movie_dict = {}\n",
    "    for j in ratings_df[ratings_df['userId']==i]['movieId'].tolist():\n",
    "        movie_dict[j] = 1\n",
    "    user_movies_dict[i]= movie_dict\n",
    "#     (ratings_df[ratings_df['userId']==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_movies_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_movies_dict[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "a = csr_matrix([0,0,0,1])\n",
    "a[0,1]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.todense()\n",
    "print(ratings_df['userId'].unique().shape[0])\n",
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sparse = tf.sparse.SparseTensor(indices=[[0,0]], values=[1],dense_shape=[1000000,1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_indices = np.array([range(1,26622)])\n",
    "movie_indices = np.repeat(movie_indices, 64, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse= csr_matrix(([1], ([0], [0])), shape=(25000000, 62000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sparse.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp[0,2] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse[0]=sp.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil= lil_matrix((100000, 62000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil[0,2]=3\n",
    "lil[0,4]=1\n",
    "lil[0,5]=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_indices = np.array([range(0,x_train.shape[1])])\n",
    "movie_indices = np.repeat(movie_indices, 128, axis = 0)\n",
    "def nn_batch_generator(x, y, batch_size, samples_per_epoch):\n",
    "    \n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    x =  x[shuffle_index, :]\n",
    "    y =  y[shuffle_index, :]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        x_batch = np.array(x[index_batch,:].todense()).astype(float)\n",
    "        x_new_batch = x_batch*movie_indices\n",
    "        \n",
    "        counter += 1\n",
    "        yield ([x_new_batch, x_batch], x_batch)\n",
    "        if (counter >= number_of_batches):\n",
    "            counter=0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator =nn_batch_generator(x_train, x_train, 128, x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 130/1269 [01:57<13:25,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i,j in tqdm(generator, total=int(x_train.shape[0]/128)):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5df2ac17b8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-fa575c554529>\u001b[0m in \u001b[0;36mnn_batch_generator\u001b[0;34m(x, y, batch_size, samples_per_epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mindex_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx_new_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmovie_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,j in generator:\n",
    "    print(len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MOVIE EMBEDDINGS</h1>\n",
    "<h2>MVAE</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('clean/MovieFeatureVector.csv')\n",
    "feature_df.pop('tmdbId')\n",
    "feature_df = feature_df.set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>af</th>\n",
       "      <th>ak</th>\n",
       "      <th>am</th>\n",
       "      <th>ar</th>\n",
       "      <th>as</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>bg</th>\n",
       "      <th>bm</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026189</td>\n",
       "      <td>0.076369</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>-0.069810</td>\n",
       "      <td>-0.107645</td>\n",
       "      <td>-0.013949</td>\n",
       "      <td>-0.042618</td>\n",
       "      <td>0.039779</td>\n",
       "      <td>-0.029764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116423</td>\n",
       "      <td>0.049320</td>\n",
       "      <td>-0.130721</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.040562</td>\n",
       "      <td>-0.027616</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>-0.008199</td>\n",
       "      <td>-0.025869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018968</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.119756</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>-0.056118</td>\n",
       "      <td>-0.038796</td>\n",
       "      <td>-0.042909</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.032308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.035324</td>\n",
       "      <td>-0.116697</td>\n",
       "      <td>-0.012555</td>\n",
       "      <td>-0.063151</td>\n",
       "      <td>-0.059833</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.041529</td>\n",
       "      <td>-0.017190</td>\n",
       "      <td>-0.033720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108175</td>\n",
       "      <td>-0.023547</td>\n",
       "      <td>-0.139049</td>\n",
       "      <td>-0.065952</td>\n",
       "      <td>0.046830</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.027590</td>\n",
       "      <td>0.051061</td>\n",
       "      <td>0.021861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055698</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>-0.129931</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.061127</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.056713</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>-0.013415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047187</td>\n",
       "      <td>0.026757</td>\n",
       "      <td>-0.096046</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>-0.052543</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.015554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209163</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081694</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>-0.096148</td>\n",
       "      <td>-0.013727</td>\n",
       "      <td>-0.075042</td>\n",
       "      <td>-0.067631</td>\n",
       "      <td>-0.006398</td>\n",
       "      <td>-0.015436</td>\n",
       "      <td>0.072134</td>\n",
       "      <td>-0.044202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036683</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.121118</td>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.090418</td>\n",
       "      <td>-0.015226</td>\n",
       "      <td>-0.034210</td>\n",
       "      <td>-0.028699</td>\n",
       "      <td>-0.010553</td>\n",
       "      <td>0.002151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095261</td>\n",
       "      <td>-0.010884</td>\n",
       "      <td>-0.087406</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>-0.099331</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>0.035535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62000 rows  411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ab   af   ak   am   ar   as   ay   az   bg   bm  ...       290  \\\n",
       "movieId                                                    ...             \n",
       "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026189   \n",
       "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.116423   \n",
       "3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.018968   \n",
       "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000760   \n",
       "5        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.108175   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "209157   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.055698   \n",
       "209159   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.047187   \n",
       "209163   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.081694   \n",
       "209169   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.036683   \n",
       "209171   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.095261   \n",
       "\n",
       "              291       292       293       294       295       296       297  \\\n",
       "movieId                                                                         \n",
       "1        0.076369 -0.112352  0.019649 -0.069810 -0.107645 -0.013949 -0.042618   \n",
       "2        0.049320 -0.130721 -0.000997 -0.040562 -0.027616  0.000512  0.010408   \n",
       "3       -0.003209 -0.119756  0.024853  0.037863 -0.056118 -0.038796 -0.042909   \n",
       "4        0.035324 -0.116697 -0.012555 -0.063151 -0.059833 -0.017836 -0.041529   \n",
       "5       -0.023547 -0.139049 -0.065952  0.046830  0.004845 -0.008362 -0.027590   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "209157   0.041496 -0.129931 -0.003363 -0.061127  0.003019 -0.000026 -0.056713   \n",
       "209159   0.026757 -0.096046  0.005746 -0.052543  0.011463  0.032719  0.012167   \n",
       "209163   0.056029 -0.096148 -0.013727 -0.075042 -0.067631 -0.006398 -0.015436   \n",
       "209169   0.002168 -0.121118 -0.041787 -0.090418 -0.015226 -0.034210 -0.028699   \n",
       "209171  -0.010884 -0.087406  0.009781  0.003008  0.023415  0.002597 -0.099331   \n",
       "\n",
       "              298       299  \n",
       "movieId                      \n",
       "1        0.039779 -0.029764  \n",
       "2       -0.008199 -0.025869  \n",
       "3        0.031052  0.032308  \n",
       "4       -0.017190 -0.033720  \n",
       "5        0.051061  0.021861  \n",
       "...           ...       ...  \n",
       "209157   0.052466 -0.013415  \n",
       "209159   0.093388  0.015554  \n",
       "209163   0.072134 -0.044202  \n",
       "209169  -0.010553  0.002151  \n",
       "209171   0.026317  0.035535  \n",
       "\n",
       "[62000 rows x 411 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Index in  movie feature vector</h2>\n",
    "<ul>\n",
    "<li>0:106 Languages</li>\n",
    "<li>107 adult</li>\n",
    "<li>108 voteaverage</li>\n",
    "<li>109:111 VAD</li>\n",
    "<li>112:411 Word2Vec</li>\n",
    "</ul>\n",
    "\n",
    "<p>Total dimension of movie feature vector 412</p>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>af</th>\n",
       "      <th>ak</th>\n",
       "      <th>am</th>\n",
       "      <th>ar</th>\n",
       "      <th>as</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>bg</th>\n",
       "      <th>bm</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026189</td>\n",
       "      <td>0.076369</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>-0.069810</td>\n",
       "      <td>-0.107645</td>\n",
       "      <td>-0.013949</td>\n",
       "      <td>-0.042618</td>\n",
       "      <td>0.039779</td>\n",
       "      <td>-0.029764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116423</td>\n",
       "      <td>0.049320</td>\n",
       "      <td>-0.130721</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.040562</td>\n",
       "      <td>-0.027616</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>-0.008199</td>\n",
       "      <td>-0.025869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018968</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.119756</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>-0.056118</td>\n",
       "      <td>-0.038796</td>\n",
       "      <td>-0.042909</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.032308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.035324</td>\n",
       "      <td>-0.116697</td>\n",
       "      <td>-0.012555</td>\n",
       "      <td>-0.063151</td>\n",
       "      <td>-0.059833</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.041529</td>\n",
       "      <td>-0.017190</td>\n",
       "      <td>-0.033720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108175</td>\n",
       "      <td>-0.023547</td>\n",
       "      <td>-0.139049</td>\n",
       "      <td>-0.065952</td>\n",
       "      <td>0.046830</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.027590</td>\n",
       "      <td>0.051061</td>\n",
       "      <td>0.021861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055698</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>-0.129931</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.061127</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.056713</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>-0.013415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047187</td>\n",
       "      <td>0.026757</td>\n",
       "      <td>-0.096046</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>-0.052543</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.015554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209163</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081694</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>-0.096148</td>\n",
       "      <td>-0.013727</td>\n",
       "      <td>-0.075042</td>\n",
       "      <td>-0.067631</td>\n",
       "      <td>-0.006398</td>\n",
       "      <td>-0.015436</td>\n",
       "      <td>0.072134</td>\n",
       "      <td>-0.044202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036683</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.121118</td>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.090418</td>\n",
       "      <td>-0.015226</td>\n",
       "      <td>-0.034210</td>\n",
       "      <td>-0.028699</td>\n",
       "      <td>-0.010553</td>\n",
       "      <td>0.002151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095261</td>\n",
       "      <td>-0.010884</td>\n",
       "      <td>-0.087406</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>-0.099331</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>0.035535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62000 rows  411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ab   af   ak   am   ar   as   ay   az   bg   bm  ...       290  \\\n",
       "movieId                                                    ...             \n",
       "1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026189   \n",
       "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.116423   \n",
       "3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.018968   \n",
       "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000760   \n",
       "5        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.108175   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "209157   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.055698   \n",
       "209159   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.047187   \n",
       "209163   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.081694   \n",
       "209169   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.036683   \n",
       "209171   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.095261   \n",
       "\n",
       "              291       292       293       294       295       296       297  \\\n",
       "movieId                                                                         \n",
       "1        0.076369 -0.112352  0.019649 -0.069810 -0.107645 -0.013949 -0.042618   \n",
       "2        0.049320 -0.130721 -0.000997 -0.040562 -0.027616  0.000512  0.010408   \n",
       "3       -0.003209 -0.119756  0.024853  0.037863 -0.056118 -0.038796 -0.042909   \n",
       "4        0.035324 -0.116697 -0.012555 -0.063151 -0.059833 -0.017836 -0.041529   \n",
       "5       -0.023547 -0.139049 -0.065952  0.046830  0.004845 -0.008362 -0.027590   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "209157   0.041496 -0.129931 -0.003363 -0.061127  0.003019 -0.000026 -0.056713   \n",
       "209159   0.026757 -0.096046  0.005746 -0.052543  0.011463  0.032719  0.012167   \n",
       "209163   0.056029 -0.096148 -0.013727 -0.075042 -0.067631 -0.006398 -0.015436   \n",
       "209169   0.002168 -0.121118 -0.041787 -0.090418 -0.015226 -0.034210 -0.028699   \n",
       "209171  -0.010884 -0.087406  0.009781  0.003008  0.023415  0.002597 -0.099331   \n",
       "\n",
       "              298       299  \n",
       "movieId                      \n",
       "1        0.039779 -0.029764  \n",
       "2       -0.008199 -0.025869  \n",
       "3        0.031052  0.032308  \n",
       "4       -0.017190 -0.033720  \n",
       "5        0.051061  0.021861  \n",
       "...           ...       ...  \n",
       "209157   0.052466 -0.013415  \n",
       "209159   0.093388  0.015554  \n",
       "209163   0.072134 -0.044202  \n",
       "209169  -0.010553  0.002151  \n",
       "209171   0.026317  0.035535  \n",
       "\n",
       "[62000 rows x 411 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(feature_df.iloc[:,0:106])\n",
    "# print(feature_df.iloc[:,106:107])\n",
    "# print(feature_df.iloc[:,107:108])\n",
    "# print(feature_df.iloc[:,108:111])\n",
    "feature_df.iloc[:,0:411]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_feature = feature_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_feature.shape\n",
    "INPUT_SIZE = movie_feature.shape[1]\n",
    "INTERMEDIATE_DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Better I guess</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding movie.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps movie vector to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded movie vector, back into a movie feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        ,axis=1))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "\n",
    "# custom loss function with tf.nn.sigmoid_cross_entropy_with_logits\n",
    "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                            labels=x_true, logits=x_recons_logits)\n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1])\n",
    "    return tf.math.reduce_mean(neg_log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=60000\n",
    "validation_size=10000\n",
    "np.random.shuffle(movie_feature)\n",
    "train_dataset= movie_feature[:train_size]\n",
    "validation_dataset =  movie_feature[train_size:train_size+validation_size]\n",
    "print(train_dataset.shape)\n",
    "validation_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_DIM = 50\n",
    "LATENT_DIM = 3\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "vae = VariationalAutoEncoder(INPUT_SIZE, INTERMEDIATE_DIM, LATENT_DIM, name='MVAE')\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "vae.compile(optimizer, loss=custom_sigmoid_cross_entropy_loss_with_logits)\n",
    "train_history = vae.fit(train_dataset, train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(validation_dataset, validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss_train = train_history.history['loss']\n",
    "loss_val = train_history.history['val_loss']\n",
    "epochs = range(EPOCHS)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataset:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,predicted= vae.encoder(movie_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_np = predicted.numpy()\n",
    "np.save('tmp/embed_movie',predicted_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_np_load = np.load('tmp/embed_movie.npy')\n",
    "predicted_np_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,p = vae.encoder(movie_feature[2].reshape(1,411))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
